{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skylercain/code/cs-6640-artificial-neural-networks/.direnv/python-3.11/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 97ms/step - accuracy: 0.1681 - loss: 2.9412 - val_accuracy: 0.3066 - val_loss: 2.3731\n",
      "Epoch 2/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m49s\u001B[0m 95ms/step - accuracy: 0.3063 - loss: 2.3415 - val_accuracy: 0.3779 - val_loss: 2.1394\n",
      "Epoch 3/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 100ms/step - accuracy: 0.3732 - loss: 2.1140 - val_accuracy: 0.4026 - val_loss: 2.0424\n",
      "Epoch 4/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 96ms/step - accuracy: 0.4086 - loss: 1.9774 - val_accuracy: 0.4319 - val_loss: 1.9060\n",
      "Epoch 5/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 99ms/step - accuracy: 0.4483 - loss: 1.8578 - val_accuracy: 0.4477 - val_loss: 1.8910\n",
      "Epoch 6/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 97ms/step - accuracy: 0.4752 - loss: 1.7557 - val_accuracy: 0.4400 - val_loss: 1.8745\n",
      "Epoch 7/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m53s\u001B[0m 101ms/step - accuracy: 0.4958 - loss: 1.6762 - val_accuracy: 0.4640 - val_loss: 1.7642\n",
      "Epoch 8/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 98ms/step - accuracy: 0.5218 - loss: 1.6064 - val_accuracy: 0.4827 - val_loss: 1.7423\n",
      "Epoch 9/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m55s\u001B[0m 106ms/step - accuracy: 0.5373 - loss: 1.5217 - val_accuracy: 0.4873 - val_loss: 1.7231\n",
      "Epoch 10/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m53s\u001B[0m 102ms/step - accuracy: 0.5656 - loss: 1.4396 - val_accuracy: 0.4813 - val_loss: 1.7200\n",
      "Epoch 11/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m53s\u001B[0m 101ms/step - accuracy: 0.5706 - loss: 1.3759 - val_accuracy: 0.4947 - val_loss: 1.6981\n",
      "Epoch 12/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 99ms/step - accuracy: 0.5903 - loss: 1.3137 - val_accuracy: 0.4957 - val_loss: 1.7101\n",
      "Epoch 13/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m52s\u001B[0m 99ms/step - accuracy: 0.6042 - loss: 1.2598 - val_accuracy: 0.4928 - val_loss: 1.7446\n",
      "Epoch 14/20\n",
      "\u001B[1m521/521\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m51s\u001B[0m 98ms/step - accuracy: 0.6234 - loss: 1.2138 - val_accuracy: 0.4930 - val_loss: 1.7942\n",
      "\u001B[1m321/321\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 34ms/step - accuracy: 0.5054 - loss: 1.6507\n",
      "Test Accuracy: 0.5049688220024109\n"
     ]
    }
   ],
   "source": [
    "# Bible Verse Classification using Bidirectional LSTM and Pre-trained Embeddings\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/kjvdata.csv\")\n",
    "\n",
    "# Mapping book to author\n",
    "author_list = {\n",
    "    \"Genesis\": \"Moses\",\n",
    "    \"Exodus\": \"Moses\",\n",
    "    \"Leviticus\": \"Moses\",\n",
    "    \"Numbers\": \"Moses\",\n",
    "    \"Deuteronomy\": \"Moses\",\n",
    "    \"Joshua\": \"Joshua\",\n",
    "    \"Judges\": \"Samuel, Nathan, Gad\",\n",
    "    \"Ruth\": \"Samuel, Nathan, Gad\",\n",
    "    \"1 Samuel (1 Kings)\": \"Samuel, Nathan, Gad\",\n",
    "    \"2 Samuel (2 Kings)\": \"Samuel, Nathan, Gad\",\n",
    "    \"1 Kings (3 Kings)\": \"Jeremiah\",\n",
    "    \"2 Kings (4 Kings)\": \"Jeremiah\",\n",
    "    \"1 Chronicles\": \"Ezra\",\n",
    "    \"2 Chronicles\": \"Ezra\",\n",
    "    \"Ezra\": \"Ezra\",\n",
    "    \"Nehemiah\": \"Nehemiah, Ezra\",\n",
    "    \"Esther\": \"Mordecai\",\n",
    "    \"Job\": \"Job,Moses\",\n",
    "    \"Psalms\": \"David,Asaph, Ezra, the sons of Korah, Heman, Ethan, Moses\",\n",
    "    \"Proverbs\": \"Solomon ,Agur(30) and Lemuel(31)\",\n",
    "    \"Ecclesiastes\": \"Solomon\",\n",
    "    \"Song of Solomon (Canticles)\": \"Solomon\",\n",
    "    \"Isaiah\": \"Isaiah\",\n",
    "    \"Jeremiah\": \"Jeremiah\",\n",
    "    \"Lamentations\": \"Jeremiah\",\n",
    "    \"Ezekiel\": \"Ezekiel\",\n",
    "    \"Daniel\": \"Daniel\",\n",
    "    \"Hosea\": \"Hosea\",\n",
    "    \"Joel\": \"Joel\",\n",
    "    \"Amos\": \"Amos\",\n",
    "    \"Obadiah\": \"Obadiah\",\n",
    "    \"Jonah\": \"Jonah\",\n",
    "    \"Micah\": \"Micah\",\n",
    "    \"Nahum\": \"Nahum\",\n",
    "    \"Habakkuk\": \"Habakkuk\",\n",
    "    \"Zephaniah\": \"Zephaniah\",\n",
    "    \"Haggai\": \"Haggai\",\n",
    "    \"Zechariah\": \" Zechariah\",\n",
    "    \"Malachi\": \"Malachi\",\n",
    "    \"Matthew\": \"Matthew\",\n",
    "    \"Mark\": \"John Mark\",\n",
    "    \"Luke\": \"Luke\",\n",
    "    \"John\": \"John, the Apostle\",\n",
    "    \"Acts\": \"Luke\",\n",
    "    \"Romans\": \"Paul\",\n",
    "    \"1 Corinthians\": \"Paul\",\n",
    "    \"2 Corinthians\": \"Paul\",\n",
    "    \"Galatians\": \"Paul\",\n",
    "    \"Ephesians\": \"Paul\",\n",
    "    \"Philippians\": \"Paul\",\n",
    "    \"Colossians\": \"Paul\",\n",
    "    \"1 Thessalonians\": \"Paul\",\n",
    "    \"2 Thessalonians\": \"Paul\",\n",
    "    \"1 Timothy\": \"Paul\",\n",
    "    \"2 Timothy\": \"Paul\",\n",
    "    \"Titus\": \"Paul\",\n",
    "    \"Philemon\": \"Paul\",\n",
    "    \"Hebrews\": \"Paul, Luke, Barnabas, Apollos\",\n",
    "    \"James\": \"James the brother of Jesus and Jude (not the Apostle, brother of John).\",\n",
    "    \"1 Peter\": \"Peter\",\n",
    "    \"2 Peter\": \"Peter\",\n",
    "    \"1 John\": \"John, the Apostle\",\n",
    "    \"2 John\": \"John, the Apostle\",\n",
    "    \"3 John\": \"John, the Apostle\",\n",
    "    \"Jude\": \"Jude, the brother of Jesus\",\n",
    "    \"Revelation\": \"John, the Apostle\"\n",
    "}\n",
    "\n",
    "df['author'] = df['book'].map(author_list)\n",
    "\n",
    "# Prepare features and labels\n",
    "x_features = df['text']\n",
    "y_labels = df['author']\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_features)\n",
    "sequences = tokenizer.texts_to_sequences(x_features)\n",
    "\n",
    "# Padding sequences\n",
    "max_sequence_length = 100\n",
    "x_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_labels_enc = encoder.fit_transform(y_labels)\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_padded, y_labels_enc, test_size=0.33, random_state=42)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_index = {}\n",
    "with open(\"data/glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Create embedding matrix\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Build Bidirectional LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False),\n",
    "    Bidirectional(LSTM(128, return_sequences=False)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-28T00:50:54.741653Z",
     "start_time": "2024-11-28T00:38:34.295955Z"
    }
   },
   "id": "c506063d07addecd",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a4e89e8bb5df2875"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
